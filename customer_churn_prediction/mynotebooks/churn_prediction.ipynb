{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Customer churn prediction is a critical task for businesses aiming to retain customers and reduce revenue loss. This notebook demonstrates how to build and evaluate multiple machine learning models to predict customer churn using Python's pandas and scikit-learn libraries."
      ],
      "metadata": {
        "id": "fhPbjhCztg28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXar5q521bfs"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seaborn style for better visuals\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset"
      ],
      "metadata": {
        "id": "TqJefF73t3ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('churn_data.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PFvDrD1At79m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "bIjVFM6pt-Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the dataset\n",
        "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing Values in Each Column:\\n\", missing_values)\n",
        "\n",
        "# Summary statistics\n",
        "df.describe()\n",
        "\n",
        "# Distribution of the target variable\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Distribution of Churn')\n",
        "plt.show()\n",
        "\n",
        "# Visualize distribution of numerical features\n",
        "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "df[numerical_features].hist(bins=30, figsize=(15, 5))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zCV1Sne02oqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "Yx2VCePm2x3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'customerID' as it does not contribute to churn prediction\n",
        "df.drop('customerID', axis=1, inplace=True)\n",
        "\n",
        "# Handle missing values in 'TotalCharges'\n",
        "# Convert 'TotalCharges' to numeric, coerce errors to NaN\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Check missing values after conversion\n",
        "print(\"\\nMissing Values After Conversion:\\n\", df.isnull().sum())\n",
        "\n",
        "# Fill missing 'TotalCharges' with median\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"\\nMissing Values After Imputation:\\n\", df.isnull().sum())\n",
        "\n",
        "# Encode 'gender' column\n",
        "le = LabelEncoder()\n",
        "df['gender'] = le.fit_transform(df['gender'])  # Female=0, Male=1\n",
        "\n",
        "# Encode binary categorical variables\n",
        "binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Encode 'MultipleLines' - considering 'No phone service' as separate category\n",
        "df['MultipleLines'] = df['MultipleLines'].replace({'No phone service': 'No'})\n",
        "df['MultipleLines'] = df['MultipleLines'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Encode 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "# 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod'\n",
        "\n",
        "# List of categorical columns to encode\n",
        "categorical_cols = ['InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
        "                    'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "                    'StreamingMovies', 'Contract', 'PaymentMethod']\n",
        "\n",
        "# One-Hot Encoding for categorical variables\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Feature Scaling for numerical features\n",
        "scaler = StandardScaler()\n",
        "df[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.fit_transform(df[['tenure', 'MonthlyCharges', 'TotalCharges']])\n",
        "\n",
        "# Display the preprocessed data\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "UxPbb-k42173"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "Ty8IrY7625Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'FamilySize' feature by combining 'Dependents' and 'Partner'\n",
        "df['FamilySize'] = df['Dependents'] + df['Partner']\n",
        "\n",
        "# Drop 'Dependents' and 'Partner' if 'FamilySize' is sufficient\n",
        "df.drop(['Dependents', 'Partner'], axis=1, inplace=True)\n",
        "\n",
        "# Create 'HasMultipleServices' feature\n",
        "service_cols = ['PhoneService', 'MultipleLines', 'InternetService_Fiber optic',\n",
        "               'InternetService_No', 'OnlineSecurity_Yes', 'OnlineBackup_Yes',\n",
        "               'DeviceProtection_Yes', 'TechSupport_Yes', 'StreamingTV_Yes',\n",
        "               'StreamingMovies_Yes']\n",
        "df['HasMultipleServices'] = df[service_cols].sum(axis=1)\n",
        "\n",
        "# Drop individual service columns if 'HasMultipleServices' is sufficient\n",
        "df.drop(service_cols, axis=1, inplace=True)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "xPi3usnI3FoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data"
      ],
      "metadata": {
        "id": "GbBF8Lbe3KPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target variable\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Split the data with stratification to maintain class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Display the shape of the splits\n",
        "print(f\"Training set: {X_train.shape}, Testing set: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "sWLXkL-837TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training and Evaluation"
      ],
      "metadata": {
        "id": "3hOc4Rqx38C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Logistic Regression\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_logreg))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_logreg))\n"
      ],
      "metadata": {
        "id": "D8IhIlfi3-2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Classifier\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "dlE87PxF4IoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Support Vector Machine\n",
        "svm = SVC(random_state=42, probability=True)\n",
        "\n",
        "# Train the model\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Support Vector Machine\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, svm.predict_proba(X_test)[:,1]))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "qoLhD9DQ4LrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize K-Nearest Neighbors\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"K-Nearest Neighbors\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, knn.predict_proba(X_test)[:,1]))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "id": "Uw9z7-ky4OF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gradient Boosting Classifier\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Gradient Boosting Classifier\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, gb.predict_proba(X_test)[:,1]))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_gb))\n"
      ],
      "metadata": {
        "id": "uhblkVh54QAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Models"
      ],
      "metadata": {
        "id": "GzacJzhV4SW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store evaluation metrics\n",
        "models = ['Logistic Regression', 'Random Forest', 'SVM', 'KNN', 'Gradient Boosting']\n",
        "accuracy = [\n",
        "    accuracy_score(y_test, y_pred_logreg),\n",
        "    accuracy_score(y_test, y_pred_rf),\n",
        "    accuracy_score(y_test, y_pred_svm),\n",
        "    accuracy_score(y_test, y_pred_knn),\n",
        "    accuracy_score(y_test, y_pred_gb)\n",
        "]\n",
        "roc_auc = [\n",
        "    roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1]),\n",
        "    roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]),\n",
        "    roc_auc_score(y_test, svm.predict_proba(X_test)[:,1]),\n",
        "    roc_auc_score(y_test, knn.predict_proba(X_test)[:,1]),\n",
        "    roc_auc_score(y_test, gb.predict_proba(X_test)[:,1])\n",
        "]\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracy,\n",
        "    'ROC-AUC': roc_auc\n",
        "})\n",
        "\n",
        "# Display the comparison\n",
        "comparison_df\n"
      ],
      "metadata": {
        "id": "36MaTgVH4USX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualization\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Plot Accuracy\n",
        "sns.barplot(x='Model', y='Accuracy', data=comparison_df, palette='viridis')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC-AUC\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Model', y='ROC-AUC', data=comparison_df, palette='magma')\n",
        "plt.title('Model ROC-AUC Comparison')\n",
        "plt.ylabel('ROC-AUC Score')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2aJxOa_w4WAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "__nT9O8u4a8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
        "\n",
        "# Best estimator\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Predict with the best estimator\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "print(\"Tuned Random Forest Classifier\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best_rf))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, best_rf.predict_proba(X_test)[:,1]))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best_rf))\n"
      ],
      "metadata": {
        "id": "WLI7aEdO4dYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "RNjKH37R4fkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from Random Forest\n",
        "importances = best_rf.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Display top 10 important features\n",
        "top_features = feature_importances.head(10)\n",
        "top_features\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_features, y=top_features.index, palette='coolwarm')\n",
        "plt.title('Top 10 Feature Importances (Random Forest)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WOelrpgK4hz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}