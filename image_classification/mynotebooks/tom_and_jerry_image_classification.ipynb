{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3682957,
          "sourceType": "datasetVersion",
          "datasetId": 2136537
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'tom-and-jerry-image-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2136537%2F3682957%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240922%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240922T111302Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5f747b23f1aac2030748c2c9bfc7bcdcb03677ea459ffc0d150bab9b6b53dfabe07ac7ebe1978619841049051aba0d6428ea89bf301453545e0d21eb8f8484d1dfae9377eda76e1177bc59c5c9757379d6a2d4ae08022482ea0dc963911adde7c46d6b4f11ebe728cb626a8f214e231623a9bfa18baceae15a69ba996feda8e7d8f88444e65a05f3a7b70e9cd68dfbb5d058c35adc264a3b62e5a7447ef3a973e68efa264b51b771367439312f97f571d056e5ecf5db5fdd0b032105ae4a948145ebcb949d047eb53849cacb3434185c0156e2de2d230e8d13b1aa2e5bc2477603fae7ea887b3aace8ecb52b381a6188e70d27b64a342c52a8908965627a6597'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Bv-k6Zfw854n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18da8670-7479-472b-ad1e-b1aa817fe165"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading tom-and-jerry-image-classification, 456031556 bytes compressed\n",
            "[==================================================] 456031556 bytes downloaded\n",
            "Downloaded and uncompressed: tom-and-jerry-image-classification\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Libraries and project"
      ],
      "metadata": {
        "id": "qhIq7jOc854p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pickle"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "DOuyAq3F854q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "XaKyj7a6854q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/tom-and-jerry-image-classification/ground_truth.csv')\n",
        "\n",
        "# Define image size and dataset folder paths\n",
        "IMG_SIZE = (128, 128)\n",
        "dataset_folder = '/kaggle/input/tom-and-jerry-image-classification/tom_and_jerry/tom_and_jerry/'\n",
        "\n",
        "# Dictionary to map subfolders to labels\n",
        "label_mapping = {\n",
        "    'tom': 0,\n",
        "    'jerry': 1,\n",
        "    'tom_jerry_1': 2,\n",
        "    'tom_jerry_0': 3\n",
        "}\n",
        "\n",
        "# Function to load images from each subfolder and assign corresponding labels\n",
        "def load_images_from_folders(dataset_folder, label_mapping):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for folder, label in label_mapping.items():\n",
        "        folder_path = os.path.join(dataset_folder, folder)\n",
        "        for filename in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(file_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, IMG_SIZE)\n",
        "                img = img / 255.0  # Normalize the image\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "            else:\n",
        "                print(f\"Image not found or could not be loaded: {file_path}\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images and corresponding labels\n",
        "images, labels = load_images_from_folders(dataset_folder, label_mapping)"
      ],
      "metadata": {
        "id": "2S-wUfJwW6sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated Random Forest and SVM"
      ],
      "metadata": {
        "id": "_3E_IPWbYoZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pickle\n",
        "\n",
        "# Function to flatten images for Random Forest and SVM\n",
        "def flatten_images(images):\n",
        "    return images.reshape(images.shape[0], -1)  # Reshape images to (num_samples, num_features)\n",
        "\n",
        "# Flatten images\n",
        "X_flattened = flatten_images(images)\n",
        "\n",
        "# Split the data into training and testing sets for RF and SVM\n",
        "X_train_flat, X_test_flat, y_train_flat, y_test_flat = train_test_split(X_flattened, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "### Random Forest Model ###\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "# Make predictions using Random Forest\n",
        "rf_pred = rf_model.predict(X_test_flat)\n",
        "\n",
        "# Evaluate the Random Forest model\n",
        "print(\"\\nRandom Forest Classifier Report:\")\n",
        "print(classification_report(y_test_flat, rf_pred))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test_flat, rf_pred)}\")\n",
        "\n",
        "# Save the Random Forest model to a pickle file\n",
        "rf_model_filename = 'rf_model_tom_and_jerry.pkl'\n",
        "with open(rf_model_filename, 'wb') as rf_file:\n",
        "    pickle.dump(rf_model, rf_file)\n",
        "print(f\"Random Forest model saved to {rf_model_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T-HgnrNcgc4",
        "outputId": "c48d67b3-1072-467f-cc64-2a1a51e357ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Classifier Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       410\n",
            "           1       0.77      0.79      0.78       231\n",
            "           2       0.88      0.76      0.82       156\n",
            "           3       0.79      0.75      0.77       299\n",
            "\n",
            "    accuracy                           0.81      1096\n",
            "   macro avg       0.82      0.80      0.81      1096\n",
            "weighted avg       0.82      0.81      0.81      1096\n",
            "\n",
            "Random Forest Accuracy: 0.8147810218978102\n",
            "Random Forest model saved to rf_model_tom_and_jerry.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### SVM Model ###\n",
        "\n",
        "# Train an SVM classifier (use a linear kernel)\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "# Make predictions using SVM\n",
        "svm_pred = svm_model.predict(X_test_flat)\n",
        "\n",
        "# Evaluate the SVM model\n",
        "print(\"\\nSVM Classifier Report:\")\n",
        "print(classification_report(y_test_flat, svm_pred))\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test_flat, svm_pred)}\")\n",
        "\n",
        "# Save the SVM model to a pickle file\n",
        "svm_model_filename = 'models/svm_model_tom_and_jerry.pkl'\n",
        "with open(svm_model_filename, 'wb') as svm_file:\n",
        "    pickle.dump(svm_model, svm_file)\n",
        "print(f\"SVM model saved to {svm_model_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jJfg_EJEdClS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preparation"
      ],
      "metadata": {
        "id": "jEq3ijJdYvd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare labels for Random Forest and SVM (flatten for binary classification)\n",
        "y_train_rf_svm = [1 if (tom == 1 or jerry == 1) else 0 for tom, jerry in y_train]\n",
        "y_test_rf_svm = [1 if (tom == 1 or jerry == 1) else 0 for tom, jerry in y_test]\n",
        "\n",
        "# Flatten the images for SVM and Random Forest (as they don't accept image tensors)\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "_NjtbQp3Yv7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM model"
      ],
      "metadata": {
        "id": "KTSVu_zrY7uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_flat, y_train_rf_svm)\n",
        "\n",
        "# Predict and evaluate SVM\n",
        "svm_pred = svm_model.predict(X_test_flat)\n",
        "print(\"\\nSVM Classifier Report:\")\n",
        "print(classification_report(y_test_rf_svm, svm_pred))\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test_rf_svm, svm_pred)}\")"
      ],
      "metadata": {
        "id": "LQC46fW7Y8A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Model"
      ],
      "metadata": {
        "id": "iMIABPTtY8Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "# One-hot encode the labels\n",
        "labels_one_hot = to_categorical(labels, num_classes=4)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a CNN model for multi-class classification\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# Add convolutional, pooling, and fully connected layers\n",
        "cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu'))\n",
        "cnn_model.add(Dense(4, activation='softmax'))  # 4 classes (Tom, Jerry, both, neither)\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Save the CNN model for Flask deployment\n",
        "cnn_model.save('models/cnn_model_tom_and_jerry.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seH2ZVUoY8lr",
        "outputId": "6cf645d6-d32c-4c26-b1ef-8b6d9dc656cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 1s/step - accuracy: 0.3666 - loss: 1.3060 - val_accuracy: 0.5137 - val_loss: 1.1324\n",
            "Epoch 2/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 1s/step - accuracy: 0.6223 - loss: 0.9031 - val_accuracy: 0.6861 - val_loss: 0.7996\n",
            "Epoch 3/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.7851 - loss: 0.5570 - val_accuracy: 0.7819 - val_loss: 0.6006\n",
            "Epoch 4/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.8842 - loss: 0.3476 - val_accuracy: 0.8093 - val_loss: 0.6156\n",
            "Epoch 5/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.9164 - loss: 0.2392 - val_accuracy: 0.8130 - val_loss: 0.6255\n",
            "Epoch 6/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.9417 - loss: 0.1656 - val_accuracy: 0.7974 - val_loss: 0.6508\n",
            "Epoch 7/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 1s/step - accuracy: 0.9667 - loss: 0.1233 - val_accuracy: 0.7865 - val_loss: 0.8859\n",
            "Epoch 8/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.9720 - loss: 0.0896 - val_accuracy: 0.8221 - val_loss: 0.6848\n",
            "Epoch 9/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 1s/step - accuracy: 0.9801 - loss: 0.0732 - val_accuracy: 0.8266 - val_loss: 0.6454\n",
            "Epoch 10/10\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 1s/step - accuracy: 0.9826 - loss: 0.0563 - val_accuracy: 0.8221 - val_loss: 0.7409\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 268ms/step - accuracy: 0.8264 - loss: 0.7536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8220803141593933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random forest model"
      ],
      "metadata": {
        "id": "lUzvfjYBY846"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_flat, y_train_rf_svm)\n",
        "\n",
        "# Predict and evaluate Random Forest\n",
        "rf_pred = rf_model.predict(X_test_flat)\n",
        "print(\"Random Forest Classifier Report:\")\n",
        "print(classification_report(y_test_rf_svm, rf_pred))\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test_rf_svm, rf_pred)}\")"
      ],
      "metadata": {
        "id": "cuqzFmLIY9Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deploying"
      ],
      "metadata": {
        "id": "pIEBdYJPZQbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save the models\n",
        "model_dir = 'models/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)"
      ],
      "metadata": {
        "id": "lqxCN3--ZdCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Saving the Random Forest Model ###\n",
        "rf_model_filename = os.path.join(model_dir, 'rf_model.pkl')\n",
        "with open(rf_model_filename, 'wb') as rf_file:\n",
        "    pickle.dump(rf_model, rf_file)\n",
        "print(f\"Random Forest model saved to {rf_model_filename}\")\n"
      ],
      "metadata": {
        "id": "AjKq9TqnZfIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Saving the SVM Model ###\n",
        "svm_model_filename = os.path.join(model_dir, 'svm_model.pkl')\n",
        "with open(svm_model_filename, 'wb') as svm_file:\n",
        "    pickle.dump(svm_model, svm_file)\n",
        "print(f\"SVM model saved to {svm_model_filename}\")"
      ],
      "metadata": {
        "id": "WRozg1ExZgvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Saving the CNN Model ###\n",
        "# TensorFlow models are generally saved using their built-in method rather than with pickle\n",
        "cnn_model_filename = os.path.join(model_dir, 'cnn_model.h5')\n",
        "cnn_model.save(cnn_model_filename)\n",
        "print(f\"CNN model saved to {cnn_model_filename}\")"
      ],
      "metadata": {
        "id": "r9W-Do9VZhoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}